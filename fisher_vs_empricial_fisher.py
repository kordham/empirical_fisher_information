# -*- coding: utf-8 -*-
"""Fisher vs Empricial Fisher.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YoGWkCJaqF9qQ2AOZ_rATHGR8o7VkirT
"""

import torch                                                                
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import torch.nn.functional as F


def jacobian(y, x, create_graph=False):                                                               
    jac = []                                                                                          
    flat_y = y.reshape(-1)                                                                            
    grad_y = torch.zeros_like(flat_y)                                                                 
    for i in range(len(flat_y)):                                                                      
        grad_y[i] = 1.                                                                                
        grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)
        jac.append(grad_x.reshape(x.shape))                                                           
        grad_y[i] = 0.                                                                                
    return torch.stack(jac).reshape(y.shape + x.shape)                                                

#Hessian instance                                                            
def hessian(y, x):                                                                                    
    return jacobian(jacobian(y, x, create_graph=True), x)  

#Empirical Fisher instance
def get_instance_emp_fisher(y,x):
    j = jacobian(y, x, create_graph=True)
    return torch.outer(j,j)

#Create data
def initialise_data():
  n_samples = 100
  X0 = torch.tensor(np.array([[np.random.normal(loc=-1, scale=1), 
                  np.random.normal(loc=1, scale=1),
                  np.random.normal(loc=-1, scale=1),
                  np.random.normal(loc=1, scale=1)] for i in range(n_samples//2)]),requires_grad=False)


  X1 = torch.tensor(np.array([[np.random.normal(loc=1, scale=1), 
                  np.random.normal(loc=-1, scale=1),
                  np.random.normal(loc=1, scale=1),
                  np.random.normal(loc=-1, scale=1)] for i in range(n_samples//2)]),requires_grad=False)

  X = torch.cat((X0, X1),0)

  Y = torch.cat((torch.tensor(np.array([[1,0] for i in range(n_samples//2)])), 
                torch.tensor(np.array([[0,1] for i in range(n_samples//2)]))), 0)

  data = list(zip(X, Y))
  return data

data = initialise_data()
def loss_single(prediction,label):
  return -torch.log(prediction)[torch.argmax(label)]*1

n_samples= len(data)
#Model with (4,4) hidden layers
def model(x,w):
  sig = nn.Sigmoid()
  softmax = nn.Softmax(dim=0)

  layer_1 = torch.split(w,[16,24])
  w_1 = layer_1[0].reshape(4,4)
  x = sig(torch.matmul(x,w_1))

  layer_2 = torch.split(layer_1[1],[16,8])
  w_2 = layer_2[0].reshape(4,4)
  x = sig(torch.matmul(x,w_2))

  w_3 = layer_2[1].reshape(4,2)
  logits = torch.matmul(x,w_3)
  pred = softmax(logits)

  return logits,pred

def loss_avg(data,w,want_fisher=True):
  loss_tot = 0
  emp_fisher = torch.zeros((40,40))
  fisher = torch.zeros((40,40))
  for x,y in data:
      prediction = model(x,w)
      loss_tot += loss_single(prediction[1],y)

      if (want_fisher):
        #Empirical Fisher with outer product 
        emp_fisher += get_instance_emp_fisher(loss_single(prediction[1],y),w)
        #Fisher from eqn 35
        prob_n = prediction[1]
        diag_pi = torch.diag(prob_n)    
        pi_n_pi_n_T= torch.outer(prob_n,prob_n)
        jac = jacobian(prediction[0],w)
        jac_T = jac.permute(1,0)
        fisher += torch.matmul(torch.matmul(jac_T,diag_pi-pi_n_pi_n_T),jac)

  if (want_fisher):
    return loss_tot/n_samples,emp_fisher/n_samples,fisher
  else: 
    return loss_tot/n_samples


def normalise_fishers(Fishers):
    num_samples = len(Fishers)
    TrF_integral = (1 / num_samples) * np.sum(np.array([torch.trace(F) for F in Fishers]))
    return [((40) / TrF_integral) * F for F in Fishers]


n_iter = 100

#initialise variables
all_emp_fishers,all_fishers = torch.zeros((n_iter,40,40)),torch.zeros((n_iter,40,40))
EVs_fisher,EVs_emp_fisher = np.array([]),np.array([])
FR = []
Rank = []
num_epochs = 100
all_losses = torch.zeros(n_iter)
training_losses = torch.zeros((n_iter,num_epochs))
for i in range(n_iter):
  w = torch.tensor(np.random.uniform(size=(40,),low=-1.0,high=1.0),requires_grad=True)
  all_losses[i], all_emp_fishers[i],all_fishers[i] = loss_avg(data,w)
  for epoch in range(num_epochs):
      training_losses[i][epoch] = loss_avg(data,w,want_fisher = False)
      w = w - jacobian(training_losses[i][epoch],w,create_graph=False)*0.1
      print(training_losses[i][epoch])


#Normalise both Fishers
normalised_emp_fishers = normalise_fishers(all_emp_fishers)
normalised_fishers = normalise_fishers(all_fishers)

#Get Eigen Values for both for both
for F in normalised_fishers:
  EVs_fisher = np.append(EVs_fisher,torch.eig(F, eigenvectors=False,  out=None)[0][:,0].detach().numpy())

for F in normalised_emp_fishers:
  EVs_emp_fisher = np.append(EVs_emp_fisher,torch.eig(F, eigenvectors=False,  out=None)[0][:,0].detach().numpy())


print("Fisher")
x, bins, p=plt.hist(EVs_fisher, bins=5,rwidth=0.6,color='r')
for item in p:
  item.set_height(item.get_height()/(40*n_iter))
  item.set_x(item.get_x())
plt.ylim(0,1)
plt.title("")
plt.show()

x, bins, p=plt.hist(EVs_fisher, bins=5,rwidth=0.6,color='r')
for item in p:
  item.set_height(item.get_height()/(40*n_iter))
  item.set_x(item.get_x())
plt.ylim(0,1)
plt.title("Empirical fisher Eqn 35")
plt.show()

x, bins, p=plt.hist(EVs_emp_fisher, bins=5,rwidth=0.6,color='r')
for item in p:
  item.set_height(item.get_height()/(40*n_iter))
  item.set_x(item.get_x())
plt.ylim(0,1)
plt.title("Empirical fisher Amira's paper")
plt.show()

#Frobenius norm

norms = np.zeros(n_iter)
all_losses = all_losses.detach().numpy()
for i in range(n_iter):
  norms[i] = torch.linalg.norm(normalised_emp_fishers[i]-normalised_fishers[i],ord='fro').item()

plt.plot(all_losses,norms,'ro')
plt.title("Fisher vs emp Fisher - Frobenius Norm")
plt.xlabel('loss value')
plt.ylabel('Norm of F-C')
plt.yscale("log")
plt.xscale("log")
plt.show()

#Cosine Similarity:
cos = np.zeros(n_iter)
#all_losses = all_losses.detach().numpy()
for i in range(n_iter):
  cos[i] = (torch.dot(torch.diag(normalised_emp_fishers[i]),torch.diag(normalised_fishers[i]))/(torch.norm(torch.diag(normalised_fishers[i]))*torch.norm(torch.diag(normalised_emp_fishers[i])))).item()
plt.plot(all_losses,cos,'ro')
plt.title("Fisher vs emp Fisher - Cosine Similarity")
plt.xlabel('loss value')
plt.ylabel('cos similarity of diags')

plt.show()

#Trace ratios
trace_ratios = np.zeros((n_iter))

for i in range(n_iter):
  trace_ratios[i]= (torch.trace(normalised_fishers[i])/torch.trace(normalised_emp_fishers[i])).item()

plt.plot(all_losses,trace_ratios,'ro')
plt.title("Fisher vs emp Fisher - Trace Ratios")
plt.xlabel('loss value')
plt.ylabel('Tr(Fisher)/Tr(emp Fisher)')

plt.show()

err_fishers = np.mean(trace_ratios-1)

Fisher_lower_bound = all_emp_fishers - all_emp_fishers*err_fishers
Fisher_higher_bound = all_emp_fishers + all_emp_fishers*err_fishers

normalised_lower = normalise_fishers(Fisher_lower_bound)
normalised_higher = normalise_fishers(Fisher_higher_bound)

def get_entropy(all_fishers,bins=10):
  EVs = np.array([])
  for F in all_fishers:
    F = F.detach().numpy()
    val, v = np.linalg.eig(F)
    EVs = np.append(np.real(EVs),val)
  x, bins, p=plt.hist(EVs, bins=bins)
  entropy=0
  x = x/len(EVs)
  for i in x:
    if(i!=0):
      entropy-=i*np.log(i)
  return entropy

def get_entropy_difference():
  data = initialise_data()
  n_iter = 100
  

  #initialise variables
  all_emp_fishers,all_fishers = torch.zeros((n_iter,40,40)),torch.zeros((n_iter,40,40))
  
  for i in range(n_iter):
    w = torch.tensor(np.random.uniform(size=(40,),low=-1.0,high=1.0),requires_grad=True)
    loss, all_emp_fishers[i],all_fishers[i] = loss_avg(data,w)

  #Normalise both Fishers
  normalised_emp_fishers = normalise_fishers(all_emp_fishers)
  normalised_fishers = normalise_fishers(all_fishers)

  ent_emp_fisher = get_entropy(normalised_emp_fishers)
  ent_fisher = get_entropy(normalised_fishers)

  return (ent_emp_fisher-ent_fisher)/ent_emp_fisher

n_iter2=1000

entropy_errs = np.zeros((n_iter2))
for i in range(n_iter2):
  print(i)
  entropy_errs[i] = get_entropy_difference()

np.save("errors_in_entropy.npy",entropy_errs)

x, bins, p=plt.hist(entropy_errs, bins=50,rwidth=0.6,color='blue')
plt.title("Distribution of fractional errors in entropy")
plt.xlabel("fractional error")
#plt.ylabel("Probability")

for item in p:
  item.set_height(item.get_height()/len(entropy_errs)) # normalise it


plt.ylim(0,0.1)
plt.show()